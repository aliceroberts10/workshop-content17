{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "Reference: the majority of the following material was adapted from [Mark Schmidt's](https://www.cs.ubc.ca/~schmidtm/) CPSC 540 notes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "Consider a set of $i = 1, \\dots, n$ samples where each sample contains a set of $j = 1, \\dots, d$ features, $x_{ij}$, and a label $y_i$. With linear regression our label is a linear function of our features, i.e., \n",
    "$$ \\hat{y}_i=w_1x_{i1}+w_2x_{i2}+\\cdots+w_dx_{id} = \\sum_{j=1}^d w_jx_{ij} = w^Tx_i $$\n",
    "where $w_j$ are the weights or regression coefficients of $x_i$.\n",
    "\n",
    "### Least squares objective\n",
    "A common way to determine the regression coefficients is by minimizing the sum of squared errors between the predicted label ($\\hat{y}_i = w^Tx_i$) and the true label ($y_i$), i.e., \n",
    "$$f(w) = \\frac{1}{2}\\sum_{i=1}^n(w^Tx_i-y_i)^2 = \\frac{1}{2}\\begin{Vmatrix} Xw-y \\end{Vmatrix}^2 $$\n",
    "with $X \\in \\mathbb{R}^{n \\times d}$, $y \\in \\mathbb{R}^{n \\times 1}$, $w \\in \\mathbb{R}^{d \\times 1}$ and $f(w)$ is commonly referred to as the loss function. Calculating the optimal $w$ can be performed by setting the gradient to zero (i.e., $\\nabla f(w)=0$) which results in $$w = (X^TX)^{-1}(X^Ty).$$\n",
    "\n",
    "### Adding a y-intercept bias\n",
    "Given our previous linear function of features we are restricted to having predictions that pass through the origin, i.e., $\\hat{y}_i = 0$ when $x_i = 0$. This is resolved by adding a y-intercept bias, i.e., $\\hat{y}_i = \\beta + w^Tx_i$. The y-intercept bias can simply be considered an additional regression coefficient that is multiplied by one as opposed to $x_{ij}$. Incorporating this y-intercept into our original expression is as simple as adding a column of ones at the beginning of $X$ and appending $\\beta$ in $w$ such that $\\beta$ is the first regression coefficient, i.e., $w = [\\beta,w_1,\\dots,w_d]^T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Predicting red wine quality\n",
    "We will analyze a dataset containing properties of variants of the Portuguese \"Vinho Verde\" red wine. Each sample in the dataset contains a score for quality (between 0 and 10) based on sensory data and a set of measured properties including the fixed acidity, volatile acidity, citric acid content, residual sugar content, chloride content, free sulfur dioxide, total sulfur dioxide, density, pH and sulphates. Our objective is to generate a model that can provide accurate predictions of the quality score based on the measured properties. Additional information regarding this dataset can be found [here](http://archive.ics.uci.edu/ml/datasets/Wine+Quality).\n",
    "\n",
    "Source: [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "data = pd.read_csv('RedWineQuality.csv',sep=';')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select labels and features\n",
    "Y = data['quality']\n",
    "X = data.values\n",
    "X = X[:,:-1]\n",
    "\n",
    "# Collect training/testing data\n",
    "N = X.shape[0]\n",
    "print('Total samples:',N)\n",
    "perm = np.random.permutation(np.arange(N))\n",
    "Xp = X[perm]\n",
    "Yp = Y[perm]\n",
    "n = int(0.8*N)\n",
    "print('Training samples:',n)\n",
    "print('Testing samples:',N-n)\n",
    "X_train = Xp[:n,:]\n",
    "Y_train = Yp[:n]\n",
    "X_test = Xp[n:,:]\n",
    "Y_test = Yp[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform simple linear regression with intercept\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "regr = linear_model.LinearRegression()\n",
    "# Train the model\n",
    "regr.fit(X_train,Y_train)\n",
    "# Make predictions on testing datas\n",
    "Y_pred = regr.predict(X_test)\n",
    "# Analyze results\n",
    "print('Y-intercept:', regr.intercept_)\n",
    "print('Regression coefficients:', regr.coef_)\n",
    "print(\"Mean squared error: %.3f\" % mean_squared_error(Y_test,Y_pred))\n",
    "print(\"Mean absolute error: %.3f\" % mean_absolute_error(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the basis\n",
    "If we suspect that our label is a non-linear function of our features we can change our basis to include polynomials of a higher degree. For example if we have two features and want to use a quadratic basis we can have $$\\hat{y}_i = \\beta + w_1x_{i1} + w_2x^2_{i1} + w_3x_{i2} + w_4x^2_{i2}.$$\n",
    "Our original regression coefficients had $d=2$, after adding the y-intercept it became $d=3$ and finally after including the quadratic bases we have $d=5$. The new feature matrix, denoted $\\Phi(X)$, is simply an expanded version of $X$ with added columns for the quadratic terms.\n",
    "\n",
    "Using the general polynomial basis (i.e., $[1,x,x^2,\\dots,x^p]$) results in lower degree polynomials being nested within higher degree polynomials. As the degree of the general polynomial basis increases the model becomes more sensitive to the training data and therefore provides a lower training error (i.e., bias). Unfortunately high sensitivity to the training data can cause over-fitting wherein our model lacks the generality to perform well on the testing data. Using different training sets can yield a large variety of models, i.e., our model has a high variance with respect to the training data. For more information on the tradeoff between complex and general models the interested reader is referred to the [bias variance tradeoff](http://www.cs.cornell.edu/courses/cs578/2005fa/CS578.bagging.boosting.lecture.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using polynomial bases\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Repeat with different degrees\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.fit_transform(X_test)\n",
    "regr_poly = linear_model.LinearRegression()\n",
    "regr_poly.fit(X_train_poly,Y_train)\n",
    "Y_pred_poly = regr_poly.predict(X_test_poly)\n",
    "# Analyze results\n",
    "# DO NOT PRINT REGRESSION COEFFICIENTS IF degree>2\n",
    "print('Regression coefficients:', regr_poly.coef_)\n",
    "print(\"Mean squared error: %.3f\" % mean_squared_error(Y_test,Y_pred_poly))\n",
    "print(\"Mean absolute error: %.3f\" % mean_absolute_error(Y_test,Y_pred_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-Regularization\n",
    "One common method for controlling model complexity is through L2-regularization which penalizes the squared L2-norm of the regression coefficients, i.e., $\\begin{Vmatrix}w \\end{Vmatrix}^2$. Our minimization problem becomes \n",
    "$$ \\underset{w \\in \\mathbb{R}^d}{\\text{arg min}} \\quad \\frac{1}{2}\\begin{Vmatrix}Xw-y \\end{Vmatrix}^2 + \\frac{\\lambda}{2}\\begin{Vmatrix}w \\end{Vmatrix}^2$$\n",
    "where the regularization parameter $\\lambda$ is a tuning parameter that controls the weight of regularization and ultimately the resulting model complexity. Including the regularization term in our loss function, $f(w)$, and setting the gradient to zero yields the following expression for the regression coefficients:\n",
    "$$w = (X^TX+\\lambda I_d)^{-1}(X^Ty).$$\n",
    "If $n << d$ the solution for $w$ can be computed faster using an equivalent expression for $w$, i.e., \n",
    "$$ w = X^T(XX^T+\\lambda I_n)^{-1}y,$$\n",
    "because $(X^TX) \\in \\mathbb{R}^{d \\times d}$ whereas $(XX^T) \\in \\mathbb{R}^{n \\times n}$. In addition to likely improving the test error, regularization is recommended because it allows for a non-invertible $X^TX$.\n",
    "\n",
    "### Standardization\n",
    "By penalizing the L2-norm of the regression coefficients it becomes necessary to standardize the scales of the features. For example, if the values of $x_{i1}$ are on the order of $10^5$ and the values of $x_{i2}$ are on the order of $10^{-1}$, regularization will ensure that the regression coefficient $w_1$ corresponding to the large feature is forced to be small even if this degrades our model accuracy. To address this issue we standardize the scale of each feature by subtracting the mean and dividing by the standard deviation of said feature, i.e.,\n",
    "$$x_{ij} = \\frac{x_{ij}-\\mu_j}{\\sigma_j},$$\n",
    "where \n",
    "$$ \\mu_j = \\frac{1}{n}\\sum_{i=1}^n x_{ij} \\quad \\text{and} \\quad \\sigma_j = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\mu_j)^2}.$$\n",
    "This standardization ensures that regularization of $w$ has a similar effect on each feature. Similarly, the scale of the label $y_i$ is often standardized such that it is on the same scale as the standardized features, i.e.,\n",
    "$$y_i = \\frac{y_i-\\mu_y}{\\sigma_y}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "X_train_std = preprocessing.scale(X_train)\n",
    "X_test_std = preprocessing.scale(X_test)\n",
    "#Y_train_std = preprocessing.scale(Y_train)\n",
    "#Y_test_std = preprocessing.scale(Y_test)\n",
    "#Create polynomial bases\n",
    "X_train_poly_std = poly.fit_transform(X_train_std)\n",
    "X_test_poly_std = poly.fit_transform(X_test_std)\n",
    "# Regularization with standardized data\n",
    "regr_ridge = linear_model.Ridge(alpha=30)\n",
    "regr_poly_ridge = linear_model.Ridge(alpha=30)\n",
    "# alpha serves the role of lamdba, tinker with it to improve results\n",
    "regr_ridge.fit(X_train_std,Y_train)\n",
    "regr_poly_ridge.fit(X_train_poly_std,Y_train)\n",
    "Y_pred_ridge = regr_ridge.predict(X_test_std)\n",
    "Y_pred_poly_ridge = regr_poly_ridge.predict(X_test_poly_std)\n",
    "print(\"MSE with regularization: %.3f\" % mean_squared_error(Y_test,Y_pred_ridge))\n",
    "print(\"MAE with regularization: %.3f\" % mean_absolute_error(Y_test,Y_pred_ridge))\n",
    "print(\"MSE with polynomial bases and regularization: %.3f\" % mean_squared_error(Y_test,Y_pred_poly_ridge))\n",
    "print(\"MAE with polynomial bases and regularization: %.3f\" % mean_absolute_error(Y_test,Y_pred_poly_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets analyze our results in terms of explained variance (1 is best score, lower values are worse)\n",
    "from sklearn.metrics import explained_variance_score\n",
    "print(\"Explained variance score (ridge regression): %.3f\" % explained_variance_score(Y_test,Y_pred_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze our ridge regression coefficients\n",
    "print('Ridge regression coefficients:', regr_ridge.coef_)\n",
    "print(min(abs(regr_ridge.coef_)))\n",
    "print(max(abs(regr_ridge.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Largest absolute coefficient corresponds to alcohol content\n",
    "# Smallest absolute coefficient corresponds to citric acid content\n",
    "# Plot each of these features vs the output\n",
    "fig = plt.figure(figsize=(17,5))\n",
    "\n",
    "sp1=fig.add_subplot(1,2,1)\n",
    "sp1.plot(X[:,2],Y,'o')\n",
    "sp1.set_xlabel('Citric acid content')\n",
    "sp1.set_ylabel('Wine quality')\n",
    "\n",
    "sp2=fig.add_subplot(1,2,2)\n",
    "sp2.plot(X[:,10],Y,'o')\n",
    "sp2.set_xlabel('Alcohol content')\n",
    "sp2.set_ylabel('Wine quality')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Explained variance score (all features): %.3f\" % explained_variance_score(Y_test,Y_pred_ridge))\n",
    "# Remove the alcohol content feature from our data and fit a new model to the remaining features\n",
    "X_train_std_NA = np.delete(X_train_std,10,1)\n",
    "X_test_std_NA = np.delete(X_test_std,10,1)\n",
    "regr_ridge_NA = linear_model.Ridge(alpha=30)\n",
    "regr_ridge_NA.fit(X_train_std_NA,Y_train)\n",
    "Y_pred_ridge_NA = regr_ridge_NA.predict(X_test_std_NA)\n",
    "print(\"Explained variance score (no alcohol): %.3f\" % explained_variance_score(Y_test,Y_pred_ridge_NA))\n",
    "\n",
    "# Remove the citric acid content feature from our data and fit a new model to the remaining features\n",
    "X_train_std_NC = np.delete(X_train_std,2,1)\n",
    "X_test_std_NC = np.delete(X_test_std,2,1)\n",
    "regr_ridge_NC = linear_model.Ridge(alpha=30)\n",
    "regr_ridge_NC.fit(X_train_std_NC,Y_train)\n",
    "Y_pred_ridge_NC = regr_ridge_NC.predict(X_test_std_NC)\n",
    "print(\"Explained variance score (no citric acid): %.3f\" % explained_variance_score(Y_test,Y_pred_ridge_NC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises and additional data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise #1\n",
    "\n",
    "From a choice of $\\alpha = [0.001, 0.1, 1, 10, 100, 1000]$ determine the best choice of $\\alpha$ for minimizing the test error of predictions using ridge regression with both linear and polynomial bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the quality of white wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In WhiteWineQuality.csv you can find a set of data for predicting the quality of Portuguese white wine based on the same features as described before. \n",
    "\n",
    "#### Exercise #2 \n",
    "Load the white wine dataset, divide the data into separate training/testing sets and standardize the scale of the features.\n",
    "\n",
    "#### Exercise #3\n",
    "Using the processed white wine data, perform regression with a polynomial basis and determine the degree of the polynomial basis (from 1-5) that minimizes the test error.\n",
    "\n",
    "#### Exercise #4 \n",
    "Model the white wine data using ridge regression (linear basis) but use only 3 features. Carefully select these three features such that they maximize the explained variance of the resulting predictions. Compare the explained variance score of your 3-feature model to the explained variance score of the same type of model but with all of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Advanced Topics\n",
    "\n",
    "### Kernel Trick\n",
    "Assume we have trained our L2-regularized model with the alternative expression for $w$ introduced before. With this model we want to predict new labels $\\hat{y}$ using test data $\\hat{X}$, i.e., \n",
    "$$\\hat{y} = \\hat{X}w = \\hat{X}X^T(XX^T+\\lambda I_n)^{-1}y.$$\n",
    "Defining $K=XX^T$ and $\\hat{K} = \\hat{X}X^T$ yields\n",
    "$$\\hat{y} = \\hat{K}(K+\\lambda I_n)^{-1}y,$$\n",
    "where $K$ is commonly referred to as the Gram matrix and it contains the inner products between all of the training examples, i.e., $k(x_{i:},x_{j:}) = x^T_{i:}x_{j:}$. From the previous expression for $\\hat{y}$ we can see that computing these inner products allows us to no longer rely on $x_{i:}$ and $x_{j:}$.\n",
    "\n",
    "Recall the polynomial basis from before but now consider including feature interactions (i.e., $x_{i1}x_{i2}$, $x_{i1}x^2_{i2}$, etc.). Adding a column for each new feature results in a large feature matrix $\\Phi(X)$ with $O(d^p)$ terms where $p$ is the degree of the polynomial basis and $d$ is the number of features. The kernel trick provides an efficient method for handling problems with high degree bases and many features. For example, consider a problem with two samples, $x_{i:}$ and $x_{j:},$ and two features, $x_{i:}=(x_{i1},x_{i2})$ and $x_{j:}=(x_{j1},x_{j2})$, where we have the following second degree basis\n",
    "$$ \\phi(x_{i:})=\\begin{pmatrix}x^2_{i1},\\sqrt{2}x_{i1}x_{i2},x^2_{i2} \\end{pmatrix}.$$\n",
    "To get the Gram matrix we need to compute the inner product $\\phi(x_{i:})^T\\phi(x_{j:})$ which can be accomplished without explicitly forming $\\phi(x_{i:})$ and $\\phi(x_{j:})$, i.e.,\n",
    "$$\\begin{align} \n",
    "\\phi(x_{i:})^T\\phi(x_{j:}) &= \\begin{bmatrix}x^2_{i1} & \\sqrt{2}x_{i1}x_{i2} & x^2_{i2}  \\end{bmatrix}\\phi(x_{j:}) \\\\ \n",
    "&= x^2_{i1}x^2_{j1}+2x_{i1}x_{i2}x_{j1}x_{j2}+x^2_{i2}x^2_{j2} \\\\\n",
    "&=\\begin{pmatrix}x_{i1}x_{j1}+x_{i2}x_{j2}\\end{pmatrix}^2 = \\begin{pmatrix}\\sum_{k=1}^d x_{ik}x_{jk}\\end{pmatrix}^2 = \\begin{pmatrix}x^T_{i:}x_{j:}\\end{pmatrix}^2.\n",
    "\\end{align}$$\n",
    "If we wish to include all degree-$p$ monomials with a bias we simply raise our expression to the $p$-th power and add a constant inside the power which yields the following expressions for $K$ and $\\hat{K}$:\n",
    "$$k(x_{i:},x_{j:}) = \\begin{pmatrix}1+x^T_{i:}x_{j:}\\end{pmatrix}^p \\quad \\text{and} \\quad \\hat{k}(\\hat{x}_{i:},x_{j:}) = \\begin{pmatrix}1+\\hat{x}^T_{i:}x_{j:}\\end{pmatrix}^p.$$\n",
    "These expressions can then be used to make prediction as $\\hat{y} = \\hat{K}(K+\\lambda I_n)^{-1}y$ with a cost of $O(n^2d+n^3)$ as opposed to $O(d^p)$. Another advantage of the kernel trick is that $k(x_i,x_j)$ can be interpreted as a measure of similarity between objects which allows us to apply regression even when we don't necessarily know the features. \n",
    "\n",
    "### Radial Basis Functions\n",
    "The polynomial basis from before is an example of a parametric model wherein the size of the model is independent of the number of training examples $n$. Other parametric models include bases such as exponentials, logarithms and various trigonometric functions. Alternatively, non-parametric models grow with the number of training examples. Radial basis functions (RBFs) are a particular type of non-parametric bases that depend on distances to the training points. One of the more common kernels is the Gaussian-RBF kernel, i.e.,\n",
    "$$k(x_{i:},x_{j:}) = \\text{exp}\\begin{pmatrix}-\\frac{\\begin{Vmatrix}x_{i:}-x_{j:}\\end{Vmatrix}^2}{\\sigma^2} \\end{pmatrix},$$\n",
    "where the variance $\\sigma^2$ is a user-defined parameter that controls the relative contribution of training points $x_{j:}$ with regards to their distance from $x_{i:}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "RBF_KernelRidge = KernelRidge(alpha=0.1,kernel='rbf',gamma=0.022)\n",
    "RBF_KernelRidge.fit(X_train_std,Y_train)\n",
    "Y_pred_RBF = RBF_KernelRidge.predict(X_test_std)\n",
    "print(\"MSE with RBF kernel: %.3f\" % mean_squared_error(Y_test,Y_pred_RBF))\n",
    "print(\"MAE with RBF kernel: %.3f\" % mean_absolute_error(Y_test,Y_pred_RBF))\n",
    "print(\"Explained variance score with RBF kernel: %.3f\" % explained_variance_score(Y_test,Y_pred_RBF))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise #5\n",
    "\n",
    "From a choice of $\\alpha = [0.001, 0.1, 1, 10, 100, 1000]$ and $\\gamma = [0.001, 0.1, 1, 10, 100, 1000]$ determine the best choices of $\\alpha$ and $\\gamma$ for minimizing the MSE/MAE of predictions using kernel ridge regression (KRR) as before. Hint: Use GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
